{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08f4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dhaya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma  # Updated import\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "from typing import List, Optional\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "PATH = \"../data/ISLP_website.pdf\"\n",
    "# CHROMA_DIR = '../stats/chroma_semantic'\n",
    "# BM25_DIR = \"../stats/bm25_retriever.pkl\"\n",
    "# METADATA_DIR = \"../stats/doc_metadata.json\"\n",
    "# COLLECTION_NAME = \"agentic_rag_collection\"  # Explicit collection name\n",
    "\n",
    "# ============================================================================\n",
    "# EMBEDDINGS\n",
    "# ============================================================================\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED DOCUMENT LOADING\n",
    "# ============================================================================\n",
    "def load_documents(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents with rich metadata (LangChain 0.3+ compatible)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading documents from {file_path}\")\n",
    "    \n",
    "    loader = PyMuPDFLoader(\n",
    "        file_path=file_path,\n",
    "        extract_images=True,\n",
    "        extract_tables='markdown'\n",
    "    )\n",
    "    \n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Add rich metadata for better filtering\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc.metadata.update({\n",
    "            'page': i + 1,\n",
    "            'source': file_path,\n",
    "            'doc_id': hashlib.md5(doc.page_content.encode()).hexdigest()[:8],\n",
    "            'file_name': Path(file_path).name,\n",
    "            'total_pages': len(docs)\n",
    "        })\n",
    "    \n",
    "    logger.info(f\"✅ Loaded {len(docs)} pages\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399b3c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading documents from ../data/ISLP_website.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ Loaded 613 pages\n"
     ]
    }
   ],
   "source": [
    "docs = load_documents(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afd3ac9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">444</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>. Deep Learning\n",
       "In <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">]</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">del</span><span style=\"font-weight: bold\">(</span>Hitters,\n",
       "hit_model, hit_dm,\n",
       "hit_logger,\n",
       "hit_test, hit_train,\n",
       "X, Y,\n",
       "X_test, X_train,\n",
       "Y_test, Y_train,\n",
       "X_test_t, Y_test_t,\n",
       "hit_trainer , hit_module<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.9</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "Multilayer Network on the MNIST Digit Data\n",
       "The torchvision package comes with a number of example datasets, includ-\n",
       "ing the MNIST digit data. Our first step is to retrieve the training and test\n",
       "data sets; the <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MNIST</span><span style=\"font-weight: bold\">()</span> function within torchvision.datasets is provided for <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MNIST</span><span style=\"font-weight: bold\">()</span>\n",
       "this purpose. The data will be downloaded the first time this function is\n",
       "executed, and stored in the directory data/MNIST.\n",
       "In <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span><span style=\"font-weight: bold\">]</span>: <span style=\"font-weight: bold\">(</span>mnist_train ,\n",
       "mnist_test<span style=\"font-weight: bold\">)</span> = <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MNIST</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">root</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'data'</span>,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">train</span>=<span style=\"color: #800080; text-decoration-color: #800080\">train</span>,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">download</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">transform</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToTensor</span><span style=\"font-weight: bold\">())</span>\n",
       "for train in <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]]</span>\n",
       "mnist_train\n",
       "Out<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span><span style=\"font-weight: bold\">]</span>: Dataset MNIST\n",
       "Number of datapoints: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60000</span>\n",
       "Root location: data\n",
       "Split: Train\n",
       "StandardTransform\n",
       "Transform: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToTensor</span><span style=\"font-weight: bold\">()</span>\n",
       "There are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> images in the training data and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> in the test data.\n",
       "The images are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> × <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, and stored as a matrix of pixels. We need to\n",
       "transform each one into a vector.\n",
       "Neural networks are somewhat sensitive to the scale of the inputs, much\n",
       "as ridge and lasso regularization are affected by scaling. Here the inputs\n",
       "are eight-bit grayscale values between <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, so we rescale to the unit\n",
       "interval.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> This transformation, along with some reordering of the axes, is\n",
       "performed by the <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToTensor</span><span style=\"font-weight: bold\">()</span> transform from the torchvision.transforms\n",
       "package.\n",
       "As in our Hitters example, we form a data module from the training and\n",
       "test datasets, setting aside <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>% of the training images for validation.\n",
       "In <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span><span style=\"font-weight: bold\">]</span>: mnist_dm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SimpleDataModule</span><span style=\"font-weight: bold\">(</span>mnist_train ,\n",
       "mnist_test ,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">validation</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">num_workers</span>=<span style=\"color: #800080; text-decoration-color: #800080\">max_num_workers</span> ,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">)</span>\n",
       "26Note: eight bits means <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, which equals <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>. Since the convention is to start at <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "the possible values range from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m444\u001b[0m\n",
       "\u001b[1;36m10\u001b[0m. Deep Learning\n",
       "In \u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m: \u001b[1;35mdel\u001b[0m\u001b[1m(\u001b[0mHitters,\n",
       "hit_model, hit_dm,\n",
       "hit_logger,\n",
       "hit_test, hit_train,\n",
       "X, Y,\n",
       "X_test, X_train,\n",
       "Y_test, Y_train,\n",
       "X_test_t, Y_test_t,\n",
       "hit_trainer , hit_module\u001b[1m)\u001b[0m\n",
       "\u001b[1;36m10.9\u001b[0m.\u001b[1;36m2\u001b[0m\n",
       "Multilayer Network on the MNIST Digit Data\n",
       "The torchvision package comes with a number of example datasets, includ-\n",
       "ing the MNIST digit data. Our first step is to retrieve the training and test\n",
       "data sets; the \u001b[1;35mMNIST\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m function within torchvision.datasets is provided for \u001b[1;35mMNIST\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "this purpose. The data will be downloaded the first time this function is\n",
       "executed, and stored in the directory data/MNIST.\n",
       "In \u001b[1m[\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1m]\u001b[0m: \u001b[1m(\u001b[0mmnist_train ,\n",
       "mnist_test\u001b[1m)\u001b[0m = \u001b[1m[\u001b[0m\u001b[1;35mMNIST\u001b[0m\u001b[1m(\u001b[0m\u001b[33mroot\u001b[0m=\u001b[32m'data'\u001b[0m,\n",
       "\u001b[33mtrain\u001b[0m=\u001b[35mtrain\u001b[0m,\n",
       "\u001b[33mdownload\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "\u001b[33mtransform\u001b[0m=\u001b[1;35mToTensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "for train in \u001b[1m[\u001b[0m\u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "mnist_train\n",
       "Out\u001b[1m[\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1m]\u001b[0m: Dataset MNIST\n",
       "Number of datapoints: \u001b[1;36m60000\u001b[0m\n",
       "Root location: data\n",
       "Split: Train\n",
       "StandardTransform\n",
       "Transform: \u001b[1;35mToTensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "There are \u001b[1;36m60\u001b[0m,\u001b[1;36m000\u001b[0m images in the training data and \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m in the test data.\n",
       "The images are \u001b[1;36m28\u001b[0m × \u001b[1;36m28\u001b[0m, and stored as a matrix of pixels. We need to\n",
       "transform each one into a vector.\n",
       "Neural networks are somewhat sensitive to the scale of the inputs, much\n",
       "as ridge and lasso regularization are affected by scaling. Here the inputs\n",
       "are eight-bit grayscale values between \u001b[1;36m0\u001b[0m and \u001b[1;36m255\u001b[0m, so we rescale to the unit\n",
       "interval.\u001b[1;36m26\u001b[0m This transformation, along with some reordering of the axes, is\n",
       "performed by the \u001b[1;35mToTensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m transform from the torchvision.transforms\n",
       "package.\n",
       "As in our Hitters example, we form a data module from the training and\n",
       "test datasets, setting aside \u001b[1;36m20\u001b[0m% of the training images for validation.\n",
       "In \u001b[1m[\u001b[0m\u001b[1;36m34\u001b[0m\u001b[1m]\u001b[0m: mnist_dm = \u001b[1;35mSimpleDataModule\u001b[0m\u001b[1m(\u001b[0mmnist_train ,\n",
       "mnist_test ,\n",
       "\u001b[33mvalidation\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "\u001b[33mnum_workers\u001b[0m=\u001b[35mmax_num_workers\u001b[0m ,\n",
       "\u001b[33mbatch_size\u001b[0m=\u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m\n",
       "26Note: eight bits means \u001b[1;36m28\u001b[0m, which equals \u001b[1;36m256\u001b[0m. Since the convention is to start at \u001b[1;36m0\u001b[0m,\n",
       "the possible values range from \u001b[1;36m0\u001b[0m to \u001b[1;36m255\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "print(docs[450].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51a6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
